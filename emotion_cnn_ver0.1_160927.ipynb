{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded\n",
      "['trainlabel', 'trainimg', 'imgsize', 'testimg', 'testlabel']\n",
      "2614 train images loaded\n",
      "1743 test images loaded\n",
      "4096 dimensional input\n",
      "Image size is [64 64]\n",
      "8 classes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ryan Shin: sungjin7127@gmail.com\n",
    "#Date: 160927\n",
    "#Obj: find the best model for emotion recogntion\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import math\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "import os\n",
    "#%matplotlib inline\n",
    "print (\"Packages loaded\")\n",
    "\n",
    "#Load data\n",
    "cwd = os.getcwd()\n",
    "loadpath = cwd + \"/dataset/data_gray.npz\"\n",
    "l = np.load(loadpath)\n",
    "\n",
    "#Check what's included\n",
    "print (l.files)\n",
    "\n",
    "# Parse data\n",
    "trainimg = l['trainimg']\n",
    "trainlabel = l['trainlabel']\n",
    "testimg = l['testimg']\n",
    "testlabel = l['testlabel']\n",
    "imgsize = l['imgsize']\n",
    "#use_gray = l['use_gray']\n",
    "ntrain = trainimg.shape[0]\n",
    "nclass = trainlabel.shape[1]\n",
    "dim    = trainimg.shape[1]\n",
    "ntest  = testimg.shape[0]\n",
    "print (\"%d train images loaded\" % (ntrain))\n",
    "print (\"%d test images loaded\" % (ntest))\n",
    "print (\"%d dimensional input\" % (dim))\n",
    "print (\"Image size is %s\" % (imgsize))\n",
    "print (\"%d classes\" % (nclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (2614, 4096) (2614, 8)\n",
      "Test set (1743, 4096) (1743, 8)\n"
     ]
    }
   ],
   "source": [
    "print ('Training set', trainimg.shape, trainlabel.shape)\n",
    "#print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "print ('Test set', testimg.shape, testlabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1668b62bf0e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "image_size = 64\n",
    "num_labels = 8\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(trainimg, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print ('Training set', train_dataset.shape, train_labels.shape)\n",
    "#print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "print ('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'channel_count': 1,\n",
       " 'image_size': 64,\n",
       " 'label_count': 8,\n",
       " 'total_image_size': 4096}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"image_size\": 64,\n",
    "    \"label_count\": 8,\n",
    "    \"channel_count\": 1\n",
    "}\n",
    "datasets[\"total_image_size\"] = datasets[\"image_size\"] * datasets[\"image_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2614, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainimg.shape\n",
    "trainlabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (2614, 64, 64, 1) (2614, 1, 8)\n",
      "Test set (1743, 64, 64, 1) (1743, 1, 8)\n",
      "['total_image_size', 'train_labels', 'test_labels', 'channel_count', 'label_count', 'train', 'image_size', 'test']\n"
     ]
    }
   ],
   "source": [
    "def reformat(dataset, labels, name):\n",
    "    dataset = dataset.reshape((-1, datasets[\"image_size\"], datasets[\"image_size\"], datasets[\"channel_count\"])).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(datasets[\"label_count\"]) == labels[:,None]).astype(np.float32)\n",
    "    print(name + \" set\", dataset.shape, labels.shape)\n",
    "    return dataset, labels\n",
    "datasets[\"train\"], datasets[\"train_labels\"] = reformat(trainimg, trainlabel, \"Training\")\n",
    "#datasets[\"valid\"], datasets[\"valid_labels\"] = reformat(valid_dataset, valid_labels, \"Validation\")\n",
    "datasets[\"test\"], datasets[\"test_labels\"] = reformat(testimg, testlabel, \"Test\")\n",
    "\n",
    "print(datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_graph(graph_info, data, step_count, report_every=50):\n",
    "    with tf.Session(graph=graph_info[\"graph\"]) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        batch_size = graph_info[\"batch_size\"]\n",
    "        for step in xrange(step_count + 1):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (data[\"train_labels\"].shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = data[\"train\"][offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = data[\"train_labels\"][offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            targets = [graph_info[\"optimizer\"], graph_info[\"loss\"], graph_info[\"predictions\"]]\n",
    "            feed_dict = {graph_info[\"train\"] : batch_data, graph_info[\"labels\"] : batch_labels}\n",
    "            _, l, predictions = session.run(targets, feed_dict=feed_dict)\n",
    "            if (step % report_every == 0):\n",
    "                print(\"Minibatch loss at step\", step, \":\", l)\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                #print(\"Validation accuracy: %.1f%%\" % accuracy(graph_info[\"valid\"].eval(), data[\"valid_labels\"]))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(graph_info[\"test\"].eval(), data[\"test_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convnet_two_layer(batch_size, patch_size, depth, hidden_size, data):\n",
    "    image_size = data[\"image_size\"]\n",
    "    label_count = data[\"label_count\"]\n",
    "    channel_count = data[\"channel_count\"]\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        train = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, channel_count))\n",
    "        labels= tf.placeholder(tf.float32, shape=(batch_size, label_count))\n",
    "        #valid = tf.constant(data[\"valid\"])\n",
    "        test  = tf.constant(data[\"test\"])\n",
    "\n",
    "        # Variables.\n",
    "        layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, channel_count, depth], stddev=0.1))\n",
    "        layer1_biases  = tf.Variable(tf.zeros([depth]))\n",
    "        layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "        layer2_biases  = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "        layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, hidden_size], stddev=0.1))\n",
    "        layer3_biases  = tf.Variable(tf.constant(1.0, shape=[hidden_size]))\n",
    "        layer4_weights = tf.Variable(tf.truncated_normal([hidden_size, label_count], stddev=0.1))\n",
    "        layer4_biases  = tf.Variable(tf.constant(1.0, shape=[label_count]))\n",
    "\n",
    "          # Model.\n",
    "        def model(set):\n",
    "            conv   = tf.nn.conv2d(set, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            conv   = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            shape  = hidden.get_shape().as_list()\n",
    "            reshape= tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "            return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "        # Training computation.\n",
    "        logits = model(train)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train\": train,\n",
    "            \"labels\": labels,\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": tf.train.GradientDescentOptimizer(0.05).minimize(loss),\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"predictions\": tf.nn.softmax(logits),\n",
    "            #\"valid\": tf.nn.softmax(model(valid)),\n",
    "            \"test\":  tf.nn.softmax(model(test))\n",
    "        }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_2conv = convnet_two_layer(batch_size=16, patch_size=5, depth=16, hidden_size=64, data=datasets)\n",
    "\n",
    "run_graph(graph_2conv, datasets, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ryan Shin: sungjin7127@gmail.com\n",
    "#Date: 160927\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "print (\"Packages loaded\")\n",
    "\n",
    "#Load data\n",
    "cwd = os.getcwd()\n",
    "loadpath = cwd + \"/dataset/data_gray.npz\"\n",
    "l = np.load(loadpath)\n",
    "\n",
    "#Check what's included\n",
    "print (l.files)\n",
    "\n",
    "#Parse data\n",
    "trainimg = l['trainimg']\n",
    "trainlabel = l['trainlabel']\n",
    "testimg = l['testimg']\n",
    "testlabel = l['testlabel']\n",
    "ntrain = trainimg.shape[0]\n",
    "nclass = trainlabel.shape[1]\n",
    "dim = trainimg.shape[1]\n",
    "ntest = testimg.shape[0]\n",
    "print (\"%d train images loaded\" % (ntrain))\n",
    "print (\"%d test images loaded\" % (ntest))\n",
    "print (\"%d dimentional input\" % (dim))\n",
    "print (\"%d classes\" % (nclass))\n",
    "\n",
    "#Define network\n",
    "tf.set_random_seed(0)\n",
    "#Param. of Log. Regression\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1000\n",
    "batch_size = 10\n",
    "display_step = 100\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    #Saver init\n",
    "    ckpt_dir = './ckpt_dir'\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Create Graph for Logistic Regression\n",
    "    x = tf.placeholder(\"float\", [None, dim])\n",
    "    y = tf.placeholder(\"float\", [None, nclass])\n",
    "    W = tf.Variable(tf.zeros([dim, nclass]), name = 'weights')\n",
    "    b = tf.Variable(tf.zeros([nclass]))\n",
    "\n",
    "    #Define functions\n",
    "    WEIGHT_DECAY_FACTOR = 0.000001\n",
    "    l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                       for v in tf.trainable_variables()])\n",
    "    _pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(_pred),\n",
    "                                        reduction_indices=1))\n",
    "    cost = cost + WEIGHT_DECAY_FACTOR*l2_loss\n",
    "    #Optimizer\n",
    "    optm = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    _corr = tf.equal(tf.argmax(_pred, 1), tf.argmax(y, 1))\n",
    "    accr = tf.reduce_mean(tf.cast(_corr, tf.float32))\n",
    "    saver = tf.train.Saver()\n",
    "    print (\"Functions ready\")\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init = tf.initialize_all_variables().run()\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(session, ckpt.model_checkpoint_path) #restore all Variable\n",
    "    start = global_step.eval() # get last globak\n",
    "\n",
    "    print(\"global_step :\", start)\n",
    "    #Launch the graph\n",
    "    #sess = tf.Session()\n",
    "    #Optimize\n",
    "    #sess.run(init)\n",
    "    #Training Cycle\n",
    "    for epoch in xrange(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        num_batch = int(ntrain/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in xrange(num_batch):\n",
    "            randidx = np.random.randint(ntrain, size=batch_size)\n",
    "            batch_xs = trainimg[randidx, :]\n",
    "            batch_ys = trainlabel[randidx, :]\n",
    "            # Fit Training using batch data\n",
    "            session.run(optm, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Compute Average loss\n",
    "            avg_cost += session.run(cost,\n",
    "                                feed_dict = {x: batch_xs, y: batch_ys})/num_batch\n",
    "\n",
    "            if (epoch % 10 == 0):\n",
    "                #print('Minbath loss at step ', epoch, ':', avg_cost)\n",
    "                global_step.assign(epoch).eval()\n",
    "                saver.save(session, ckpt_dir + '/model.ckpt', global_step=global_step)\n",
    "\n",
    "            # Display logs per epoch step\n",
    "\n",
    "            if epoch % display_step == 0:\n",
    "                train_acc = session.run(accr, feed_dict={x: batch_xs, y: batch_ys})\n",
    "                print (\"Epoch: %03d/%03d cost: %.9f\" %\n",
    "                      (epoch, training_epochs, avg_cost))\n",
    "                print (\" Training accuracy: %.3f\" % (train_acc))\n",
    "                test_acc = session.run(accr, feed_dict={x: testimg, y: testlabel})\n",
    "                print (\" Test accuracy: %.3f\" % (test_acc))\n",
    "\n",
    "\n",
    "print (\"Optimization Finished\")\n",
    "#sess.close()\n",
    "print (\"Session closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK READY\n",
      "FUNCTIONS READY\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(0)\n",
    "n_input = dim\n",
    "n_output = nclass\n",
    "weights = {\n",
    "    'wc1' : tf.Variable(tf.random_normal([5,5,1,128], stddev=0.1)),\n",
    "    'wc2' : tf.Variable(tf.random_normal([5,5,128,128], stddev=0.1)),\n",
    "    'wd1' : tf.Variable(tf.random_normal(\n",
    "            [(int)(imgsize[0]/4*imgsize[1]/4)*128, 128], stddev=0.1)),\n",
    "    'wd2' : tf.Variable(tf.random_normal([128, n_output], stddev=0.1))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1' : tf.Variable(tf.random_normal([128], stddev=0.1)),\n",
    "    'bc2' : tf.Variable(tf.random_normal([128], stddev=0.1)),\n",
    "    'bd1' : tf.Variable(tf.random_normal([128], stddev=0.1)),\n",
    "    'bd2' : tf.Variable(tf.random_normal([n_output], stddev=0.1)),\n",
    "}\n",
    "\n",
    "def conv_basic(_input, _w, _b, _keepratio):\n",
    "    # INPUT\n",
    "    _input_r = tf.reshape(_input, shape=[-1, imgsize[0], imgsize[1], 1])\n",
    "    # CONVOLUTION LAYER 1\n",
    "    _conv1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(_input_r\n",
    "        , _w['wc1'], strides=[1, 1, 1, 1], padding='SAME'), _b['bc1']))\n",
    "    _pool1 = tf.nn.max_pool(_conv1, ksize=[1, 2, 2, 1]\n",
    "        , strides=[1, 2, 2, 1], padding='SAME')\n",
    "    _pool_dr1 = tf.nn.dropout(_pool1, _keepratio)\n",
    "    # CONVOLUTION LAYER 2\n",
    "    _conv2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(_pool_dr1\n",
    "        , _w['wc2'], strides=[1, 1, 1, 1], padding='SAME'), _b['bc2']))\n",
    "    _pool2 = tf.nn.max_pool(_conv2, ksize=[1, 2, 2, 1]\n",
    "        , strides=[1, 2, 2, 1], padding='SAME')\n",
    "    _pool_dr2 = tf.nn.dropout(_pool2, _keepratio)\n",
    "    # VECTORIZE\n",
    "    _dense1 = tf.reshape(_pool_dr2\n",
    "                         , [-1, _w['wd1'].get_shape().as_list()[0]])\n",
    "    # FULLY CONNECTED LAYER 1\n",
    "    _fc1 = tf.nn.relu(tf.add(tf.matmul(_dense1, _w['wd1']), _b['bd1']))\n",
    "    _fc_dr1 = tf.nn.dropout(_fc1, _keepratio)\n",
    "    # FULLY CONNECTED LAYER 2\n",
    "    _out = tf.add(tf.matmul(_fc_dr1, _w['wd2']), _b['bd2'])\n",
    "    # RETURN\n",
    "    out = {\n",
    "        'input_r': _input_r, 'conv1': _conv1, 'pool1': _pool1\n",
    "        , 'pool1_dr1': _pool_dr1, 'conv2': _conv2, 'pool2': _pool2\n",
    "        , 'pool_dr2': _pool_dr2, 'dense1': _dense1, 'fc1': _fc1\n",
    "        , 'fc_dr1': _fc_dr1, 'out': _out\n",
    "    }\n",
    "    return out\n",
    "print (\"NETWORK READY\")\n",
    "\n",
    "#Define function\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_output])\n",
    "keepratio = tf.placeholder(tf.float32)\n",
    "\n",
    "# Functions!\n",
    "_pred = conv_basic(x, weights, biases, keepratio)['out']\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(_pred, y))\n",
    "WEIGHT_DECAY_FACTOR = 0.0001\n",
    "l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "            for v in tf.trainable_variables()])\n",
    "cost = cost + WEIGHT_DECAY_FACTOR*l2_loss\n",
    "optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "_corr = tf.equal(tf.argmax(_pred,1), tf.argmax(y,1)) # Count corrects\n",
    "accr = tf.reduce_mean(tf.cast(_corr, tf.float32)) # Accuracy\n",
    "init = tf.initialize_all_variables()\n",
    "print (\"FUNCTIONS READY\")\n",
    "\n",
    "# Parameters\n",
    "training_epochs = 400\n",
    "batch_size      = 100\n",
    "display_step    = 40\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    num_batch = int(ntrain/batch_size)+1\n",
    "    # Loop over all batches\n",
    "    for i in range(num_batch):\n",
    "        randidx = np.random.randint(ntrain, size=batch_size)\n",
    "        batch_xs = trainimg[randidx, :]\n",
    "        batch_ys = trainlabel[randidx, :]\n",
    "        # Fit training using batch data\n",
    "        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys\n",
    "                                  , keepratio:0.7})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys\n",
    "                                , keepratio:1.})/num_batch\n",
    "\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0 or epoch == training_epochs-1:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" %\n",
    "               (epoch, training_epochs, avg_cost))\n",
    "        train_acc = sess.run(accr, feed_dict={x: batch_xs\n",
    "                                , y: batch_ys, keepratio:1.})\n",
    "        print (\" Training accuracy: %.3f\" % (train_acc))\n",
    "        test_acc = sess.run(accr, feed_dict={x: testimg\n",
    "                                , y: testlabel, keepratio:1.})\n",
    "        print (\" Test accuracy: %.3f\" % (test_acc))\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "sess.close()\n",
    "print (\"Session closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
